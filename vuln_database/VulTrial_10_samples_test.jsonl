{"idx": 360829, "project": "linux", "commit_id": "47abea041f897d64dbd5777f0cf7745148f85d75", "project_url": "https://github.com/torvalds/linux", "commit_url": "https://github.com/torvalds/linux/commit/47abea041f897d64dbd5777f0cf7745148f85d75", "commit_message": "io_uring: fix off-by-one in sync cancelation file check\n\nThe passed in index should be validated against the number of registered\nfiles we have, it needs to be smaller than the index value to avoid going\none beyond the end.\n\nFixes: 78a861b94959 (\"io_uring: add sync cancelation API through io_uring_register()\")\nReported-by: Luo Likang <luolikang@nsfocus.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 0, "func": "static int __io_sync_cancel(struct io_uring_task *tctx,\n\t\t\t    struct io_cancel_data *cd, int fd)\n{\n\tstruct io_ring_ctx *ctx = cd->ctx;\n\n\t/* fixed must be grabbed every time since we drop the uring_lock */\n\tif ((cd->flags & IORING_ASYNC_CANCEL_FD) &&\n\t    (cd->flags & IORING_ASYNC_CANCEL_FD_FIXED)) {\n\t\tunsigned long file_ptr;\n\n\t\tif (unlikely(fd >= ctx->nr_user_files))\n\t\t\treturn -EBADF;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tfile_ptr = io_fixed_file_slot(&ctx->file_table, fd)->file_ptr;\n\t\tcd->file = (struct file *) (file_ptr & FFS_MASK);\n\t\tif (!cd->file)\n\t\t\treturn -EBADF;\n\t}\n\n\treturn __io_async_cancel(cd, tctx, 0);\n}", "func_hash": 191453192245381781842321164119198990689, "file_name": "cancel.c", "file_hash": 191362883793091950391767824989989369401, "cwe": ["CWE-193"], "cve": "CVE-2022-3103", "cve_desc": "off-by-one in io_uring module.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-3103"}
{"idx": 464942, "project": "php-src", "commit_id": "2bcbc95f033c31b00595ed39f79c3a99b4ed0501", "project_url": "https://github.com/php/php-src", "commit_url": "http://git.php.net/?p=php-src.git;a=commit;h=2bcbc95f033c31b00595ed39f79c3a99b4ed0501", "commit_message": "Fix bug #79037 (global buffer-overflow in `mbfl_filt_conv_big5_wchar`)", "target": 0, "func": "mbfl_filt_conv_big5_wchar(int c, mbfl_convert_filter *filter)\n{\n\tint k;\n\tint c1, w, c2;\n\n\tswitch (filter->status) {\n\tcase 0:\n\t\tif (filter->from->no_encoding == mbfl_no_encoding_cp950) {\n\t\t\tc1 = 0x80;\n\t\t} else {\n\t\t\tc1 = 0xa0;\n\t\t}\n\n\t\tif (c >= 0 && c <= 0x80) {\t/* latin */\n\t\t\tCK((*filter->output_function)(c, filter->data));\n\t\t} else if (c == 0xff) {\n\t\t\tCK((*filter->output_function)(0xf8f8, filter->data));\n\t\t} else if (c > c1 && c < 0xff) {\t/* dbcs lead byte */\n\t\t\tfilter->status = 1;\n\t\t\tfilter->cache = c;\n\t\t} else {\n\t\t\tw = c & MBFL_WCSGROUP_MASK;\n\t\t\tw |= MBFL_WCSGROUP_THROUGH;\n\t\t\tCK((*filter->output_function)(w, filter->data));\n\t\t}\n\t\tbreak;\n\n\tcase 1:\t\t/* dbcs second byte */\n\t\tfilter->status = 0;\n\t\tc1 = filter->cache;\n\t\tif ((c > 0x39 && c < 0x7f) | (c > 0xa0 && c < 0xff)) {\n\t\t\tif (c < 0x7f){\n\t\t\t\tw = (c1 - 0xa1)*157 + (c - 0x40);\n\t\t\t} else {\n\t\t\t\tw = (c1 - 0xa1)*157 + (c - 0xa1) + 0x3f;\n\t\t\t}\n\t\t\tif (w >= 0 && w < big5_ucs_table_size) {\n\t\t\t\tw = big5_ucs_table[w];\n\t\t\t} else {\n\t\t\t\tw = 0;\n\t\t\t}\n\n\t\t\tif (filter->from->no_encoding == mbfl_no_encoding_cp950) {\n\t\t\t\t/* PUA for CP950 */\n\t\t\t\tif (w <= 0 && is_in_cp950_pua(c1, c)) {\n\t\t\t\t\tc2 = c1 << 8 | c;\n\t\t\t\t\tfor (k = 0; k < sizeof(cp950_pua_tbl)/(sizeof(unsigned short)*4); k++) {\n\t\t\t\t\t\tif (c2 >= cp950_pua_tbl[k][2] && c2 <= cp950_pua_tbl[k][3]) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif ((cp950_pua_tbl[k][2] & 0xff) == 0x40) {\n\t\t\t\t\t\tw = 157*(c1 - (cp950_pua_tbl[k][2]>>8)) + c - (c >= 0xa1 ? 0x62 : 0x40)\n\t\t\t\t\t\t\t+ cp950_pua_tbl[k][0];\n\t\t\t\t\t} else {\n\t\t\t\t\t\tw = c2 - cp950_pua_tbl[k][2] + cp950_pua_tbl[k][0];\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (w <= 0) {\n\t\t\t\tw = (c1 << 8) | c;\n\t\t\t\tw &= MBFL_WCSPLANE_MASK;\n\t\t\t\tw |= MBFL_WCSPLANE_BIG5;\n\t\t\t}\n\t\t\tCK((*filter->output_function)(w, filter->data));\n\t\t} else if ((c >= 0 && c < 0x21) || c == 0x7f) {\t\t/* CTLs */\n\t\t\tCK((*filter->output_function)(c, filter->data));\n\t\t} else {\n\t\t\tw = (c1 << 8) | c;\n\t\t\tw &= MBFL_WCSGROUP_MASK;\n\t\t\tw |= MBFL_WCSGROUP_THROUGH;\n\t\t\tCK((*filter->output_function)(w, filter->data));\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tfilter->status = 0;\n\t\tbreak;\n\t}\n\n\treturn c;\n}", "func_hash": 324298547167958214830404594575086371894, "file_name": "mbfilter_big5.c", "file_hash": 135358838595920427772392870415942108469, "cwe": ["CWE-125"], "cve": "CVE-2020-7060", "cve_desc": "When using certain mbstring functions to convert multibyte encodings, in PHP versions 7.2.x below 7.2.27, 7.3.x below 7.3.14 and 7.4.x below 7.4.2 it is possible to supply data that will cause function mbfl_filt_conv_big5_wchar to read past the allocated buffer. This may lead to information disclosure or crash.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2020-7060"}
{"idx": 251946, "project": "tensorflow", "commit_id": "15691e456c7dc9bd6be203b09765b063bf4a380c", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/15691e456c7dc9bd6be203b09765b063bf4a380c", "commit_message": "Prevent dereferencing of null pointers in TFLite's `add.cc`.\n\nPiperOrigin-RevId: 387244946\nChange-Id: I56094233327fbd8439b92e1dbb1262176e00eeb9", "target": 0, "func": "inline void BinaryBroadcastFiveFold(const ArithmeticParams& unswitched_params,\n                                    const RuntimeShape& unswitched_input1_shape,\n                                    const T* unswitched_input1_data,\n                                    const RuntimeShape& unswitched_input2_shape,\n                                    const T* unswitched_input2_data,\n                                    const RuntimeShape& output_shape,\n                                    T* output_data, ElementwiseF elementwise_f,\n                                    ScalarBroadcastF scalar_broadcast_f) {\n  ArithmeticParams switched_params = unswitched_params;\n  switched_params.input1_offset = unswitched_params.input2_offset;\n  switched_params.input1_multiplier = unswitched_params.input2_multiplier;\n  switched_params.input1_shift = unswitched_params.input2_shift;\n  switched_params.input2_offset = unswitched_params.input1_offset;\n  switched_params.input2_multiplier = unswitched_params.input1_multiplier;\n  switched_params.input2_shift = unswitched_params.input1_shift;\n\n  const bool use_unswitched =\n      unswitched_params.broadcast_category ==\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n\n  const ArithmeticParams& params =\n      use_unswitched ? unswitched_params : switched_params;\n  const T* input1_data =\n      use_unswitched ? unswitched_input1_data : unswitched_input2_data;\n  const T* input2_data =\n      use_unswitched ? unswitched_input2_data : unswitched_input1_data;\n\n  // Fivefold nested loops. The second input resets its position for each\n  // iteration of the second loop. The first input resets its position at the\n  // beginning of the fourth loop. The innermost loop is an elementwise add of\n  // sections of the arrays.\n  T* output_data_ptr = output_data;\n  const T* input1_data_ptr = input1_data;\n  const T* input2_data_reset = input2_data;\n  // In the fivefold pattern, y0, y2 and y4 are not broadcast, and so shared\n  // between input shapes. y3 for input 1 is always broadcast, and so the\n  // dimension there is 1, whereas optionally y1 might be broadcast for\n  // input 2. Put another way, input1.shape.FlatSize = y0 * y1 * y2 * y4,\n  // input2.shape.FlatSize = y0 * y2 * y3 * y4.\n  int y0 = params.broadcast_shape[0];\n  int y1 = params.broadcast_shape[1];\n  int y2 = params.broadcast_shape[2];\n  int y3 = params.broadcast_shape[3];\n  int y4 = params.broadcast_shape[4];\n  if (y4 > 1) {\n    // General fivefold pattern, with y4 > 1 so there is a non-broadcast inner\n    // dimension.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          for (int i3 = 0; i3 < y3; ++i3) {\n            elementwise_f(y4, params, input1_data_ptr, input2_data_ptr,\n                          output_data_ptr);\n            input2_data_ptr += y4;\n            output_data_ptr += y4;\n          }\n          // We have broadcast y4 of input1 data y3 times, and now move on.\n          input1_data_ptr += y4;\n        }\n      }\n      // We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.\n      input2_data_reset = input2_data_ptr;\n    }\n  } else if (input1_data_ptr != nullptr) {\n    // Special case of y4 == 1, in which the innermost loop is a single\n    // element and can be combined with the next (y3) as an inner broadcast.\n    //\n    // Note that this handles the case of pure scalar broadcast when\n    // y0 == y1 == y2 == 1. With low overhead it handles cases such as scalar\n    // broadcast with batch (as y2 > 1).\n    //\n    // NOTE The process is the same as the above general case except\n    // simplified for y4 == 1 and the loop over y3 is contained within the\n    // AddScalarBroadcast function.\n    for (int i0 = 0; i0 < y0; ++i0) {\n      const T* input2_data_ptr = nullptr;\n      for (int i1 = 0; i1 < y1; ++i1) {\n        input2_data_ptr = input2_data_reset;\n        for (int i2 = 0; i2 < y2; ++i2) {\n          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr,\n                             output_data_ptr);\n          input2_data_ptr += y3;\n          output_data_ptr += y3;\n          input1_data_ptr += 1;\n        }\n      }\n      input2_data_reset = input2_data_ptr;\n    }\n  }\n}", "func_hash": 63045231057409915908146244179275974599, "file_name": "optimized_ops.h", "file_hash": 177051404573399480037200295059071309532, "cwe": ["CWE-369"], "cve": "CVE-2021-37688", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can craft a TFLite model that would trigger a null pointer dereference, which would result in a crash and denial of service. The [implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L268-L285) unconditionally dereferences a pointer. We have patched the issue in GitHub commit 15691e456c7dc9bd6be203b09765b063bf4a380c. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37688"}
{"idx": 444900, "project": "cifs-utils", "commit_id": "f6eae44a3d05b6515a59651e6bed8b6dde689aec", "project_url": "https://github.com/piastry/cifs-utils", "commit_url": "http://git.samba.org/?p=cifs-utils.git;a=commitdiff;h=f6eae44a3d05b6515a59651e6bed8b6dde689aec", "commit_message": "mtab: handle ENOSPC/EFBIG condition properly when altering mtab\n\nIt's possible that when mount.cifs goes to append the mtab that there\nwon't be enough space to do so, and the mntent won't be appended to the\nfile in its entirety.\n\nAdd a my_endmntent routine that will fflush and then fsync the FILE if\nthat succeeds. If either fails then it will truncate the file back to\nits provided size. It will then call endmntent unconditionally.\n\nHave add_mtab call fstat on the opened mtab file in order to get the\nsize of the file before it has been appended. Assuming that that\nsucceeds, use my_endmntent to ensure that the file is not corrupted\nbefore closing it. It's possible that we'll have a small race window\nwhere the mtab is incorrect, but it should be quickly corrected.\n\nThis was reported some time ago as CVE-2011-1678:\n\n    http://openwall.com/lists/oss-security/2011/03/04/9\n\n...and it seems to fix the reproducer that I was able to come up with.\n\nSigned-off-by: Jeff Layton <jlayton@samba.org>\nReviewed-by: Suresh Jayaraman <sjayaraman@suse.de>", "target": 0, "func": "add_mtab(char *devname, char *mountpoint, unsigned long flags, const char *fstype)\n{\n\tint rc = 0, tmprc, fd;\n\tuid_t uid;\n\tchar *mount_user = NULL;\n\tstruct mntent mountent;\n\tstruct stat statbuf;\n\tFILE *pmntfile;\n\tsigset_t mask, oldmask;\n\n\tuid = getuid();\n\tif (uid != 0)\n\t\tmount_user = getusername(uid);\n\n\t/*\n\t * Set the real uid to the effective uid. This prevents unprivileged\n\t * users from sending signals to this process, though ^c on controlling\n\t * terminal should still work.\n\t */\n\trc = setreuid(geteuid(), -1);\n\tif (rc != 0) {\n\t\tfprintf(stderr, \"Unable to set real uid to effective uid: %s\\n\",\n\t\t\t\tstrerror(errno));\n\t\treturn EX_FILEIO;\n\t}\n\n\trc = sigfillset(&mask);\n\tif (rc) {\n\t\tfprintf(stderr, \"Unable to set filled signal mask\\n\");\n\t\treturn EX_FILEIO;\n\t}\n\n\trc = sigprocmask(SIG_SETMASK, &mask, &oldmask);\n\tif (rc) {\n\t\tfprintf(stderr, \"Unable to make process ignore signals\\n\");\n\t\treturn EX_FILEIO;\n\t}\n\n\trc = toggle_dac_capability(1, 1);\n\tif (rc)\n\t\treturn EX_FILEIO;\n\n\tatexit(unlock_mtab);\n\trc = lock_mtab();\n\tif (rc) {\n\t\tfprintf(stderr, \"cannot lock mtab\");\n\t\trc = EX_FILEIO;\n\t\tgoto add_mtab_exit;\n\t}\n\n\tpmntfile = setmntent(MOUNTED, \"a+\");\n\tif (!pmntfile) {\n\t\tfprintf(stderr, \"could not update mount table\\n\");\n\t\tunlock_mtab();\n\t\trc = EX_FILEIO;\n\t\tgoto add_mtab_exit;\n\t}\n\n\tfd = fileno(pmntfile);\n\tif (fd < 0) {\n\t\tfprintf(stderr, \"mntent does not appear to be valid\\n\");\n\t\tunlock_mtab();\n\t\trc = EX_FILEIO;\n\t\tgoto add_mtab_exit;\n\t}\n\n\trc = fstat(fd, &statbuf);\n\tif (rc != 0) {\n\t\tfprintf(stderr, \"unable to fstat open mtab\\n\");\n\t\tendmntent(pmntfile);\n\t\tunlock_mtab();\n\t\trc = EX_FILEIO;\n\t\tgoto add_mtab_exit;\n\t}\n\n\tmountent.mnt_fsname = devname;\n\tmountent.mnt_dir = mountpoint;\n\tmountent.mnt_type = (char *)(void *)fstype;\n\tmountent.mnt_opts = (char *)calloc(MTAB_OPTIONS_LEN, 1);\n\tif (mountent.mnt_opts) {\n\t\tif (flags & MS_RDONLY)\n\t\t\tstrlcat(mountent.mnt_opts, \"ro\", MTAB_OPTIONS_LEN);\n\t\telse\n\t\t\tstrlcat(mountent.mnt_opts, \"rw\", MTAB_OPTIONS_LEN);\n\n\t\tif (flags & MS_MANDLOCK)\n\t\t\tstrlcat(mountent.mnt_opts, \",mand\", MTAB_OPTIONS_LEN);\n\t\tif (flags & MS_NOEXEC)\n\t\t\tstrlcat(mountent.mnt_opts, \",noexec\", MTAB_OPTIONS_LEN);\n\t\tif (flags & MS_NOSUID)\n\t\t\tstrlcat(mountent.mnt_opts, \",nosuid\", MTAB_OPTIONS_LEN);\n\t\tif (flags & MS_NODEV)\n\t\t\tstrlcat(mountent.mnt_opts, \",nodev\", MTAB_OPTIONS_LEN);\n\t\tif (flags & MS_SYNCHRONOUS)\n\t\t\tstrlcat(mountent.mnt_opts, \",sync\", MTAB_OPTIONS_LEN);\n\t\tif (mount_user) {\n\t\t\tstrlcat(mountent.mnt_opts, \",user=\", MTAB_OPTIONS_LEN);\n\t\t\tstrlcat(mountent.mnt_opts, mount_user,\n\t\t\t\tMTAB_OPTIONS_LEN);\n\t\t}\n\t}\n\tmountent.mnt_freq = 0;\n\tmountent.mnt_passno = 0;\n\trc = addmntent(pmntfile, &mountent);\n\tif (rc) {\n\t\tfprintf(stderr, \"unable to add mount entry to mtab\\n\");\n\t\tftruncate(fd, statbuf.st_size);\n\t\trc = EX_FILEIO;\n\t}\n\ttmprc = my_endmntent(pmntfile, statbuf.st_size);\n\tif (tmprc) {\n\t\tfprintf(stderr, \"error %d detected on close of mtab\\n\", tmprc);\n\t\trc = EX_FILEIO;\n\t}\n\tunlock_mtab();\n\tSAFE_FREE(mountent.mnt_opts);\nadd_mtab_exit:\n\ttoggle_dac_capability(1, 0);\n\tsigprocmask(SIG_SETMASK, &oldmask, NULL);\n\n\treturn rc;\n}", "func_hash": 145620494366436562739751095487223645254, "file_name": "mount.cifs.c", "file_hash": 77793306769877718806811810196619780209, "cwe": ["CWE-20"], "cve": "CVE-2011-1678", "cve_desc": "smbfs in Samba 3.5.8 and earlier attempts to use (1) mount.cifs to append to the /etc/mtab file and (2) umount.cifs to append to the /etc/mtab.tmp file without first checking whether resource limits would interfere, which allows local users to trigger corruption of the /etc/mtab file via a process with a small RLIMIT_FSIZE value, a related issue to CVE-2011-1089.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2011-1678"}
{"idx": 226029, "project": "gpac", "commit_id": "64a2e1b799352ac7d7aad1989bc06e7b0f2b01db", "project_url": "https://github.com/gpac/gpac", "commit_url": "https://github.com/gpac/gpac/commit/64a2e1b799352ac7d7aad1989bc06e7b0f2b01db", "commit_message": "fixed #2092", "target": 0, "func": "\nvoid gitn_box_del(GF_Box *s)\n{\n\tu32 i;\n\tGroupIdToNameBox *ptr = (GroupIdToNameBox *)s;\n\tif (ptr == NULL) return;\n\tif (ptr->entries) {\n\t\tfor (i=0; i<ptr->nb_entries; i++) {\n\t\t\tif (ptr->entries[i].name) gf_free(ptr->entries[i].name);\n\t\t}\n\t\tgf_free(ptr->entries);\n\t}\n\tgf_free(ptr);", "func_hash": 265612189453593240220286582640125308421, "file_name": "box_code_base.c", "file_hash": 49851003819063672326837979869211393199, "cwe": ["CWE-476"], "cve": "CVE-2021-4043", "cve_desc": "NULL Pointer Dereference in GitHub repository gpac/gpac prior to 1.1.0.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-4043"}
{"idx": 483469, "project": "mongo", "commit_id": "a5e2f9b0a236462a6d1ca129583c617f111367b4", "project_url": "https://github.com/mongodb/mongo", "commit_url": "https://github.com/mongodb/mongo/commit/a5e2f9b0a236462a6d1ca129583c617f111367b4", "commit_message": "SERVER-59071 Treat '$sample' as unsharded when connecting directly to shards\n\n(cherry picked from commit f3604b901d688c194de5e430c7fbab060c9dc8e0)", "target": 0, "func": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (auto css = CollectionShardingState::get(opCtx, coll->ns());\n        css->getCollectionDescription(opCtx).isSharded() &&\n        !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // Since the incoming operation is sharded, use the CSS to infer the filtering metadata for\n        // the collection. We get the shard ownership filter after checking to see if the collection\n        // is sharded to avoid an invariant from being fired in this call.\n        auto collectionFilter = css->getOwnershipFilter(\n            opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}", "func_hash": 53510909926140160570175197476495085795, "file_name": "None", "file_hash": null, "cwe": ["CWE-617"], "cve": "CVE-2021-32037", "cve_desc": "An authorized user may trigger an invariant which may result in denial of service or server exit if a relevant aggregation request is sent to a shard. Usually, the requests are sent via mongos and special privileges are required in order to know the address of the shards and to log in to the shards of an auth enabled environment.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-32037"}
{"idx": 291761, "project": "linux", "commit_id": "8700af2cc18c919b2a83e74e0479038fd113c15d", "project_url": "https://github.com/torvalds/linux", "commit_url": "https://github.com/torvalds/linux/commit/8700af2cc18c919b2a83e74e0479038fd113c15d", "commit_message": "RDMA/rtrs-clt: Fix possible double free in error case\n\nCallback function rtrs_clt_dev_release() for put_device() calls kfree(clt)\nto free memory. We shouldn't call kfree(clt) again, and we can't use the\nclt after kfree too.\n\nReplace device_register() with device_initialize() and device_add() so that\ndev_set_name can() be used appropriately.\n\nMove mutex_destroy() to the release function so it can be called in\nthe alloc_clt err path.\n\nFixes: eab098246625 (\"RDMA/rtrs-clt: Refactor the failure cases in alloc_clt\")\nLink: https://lore.kernel.org/r/20220217030929.323849-1-haris.iqbal@ionos.com\nReported-by: Miaoqian Lin <linmq006@gmail.com>\nSigned-off-by: Md Haris Iqbal <haris.iqbal@ionos.com>\nReviewed-by: Jack Wang <jinpu.wang@ionos.com>\nSigned-off-by: Jason Gunthorpe <jgg@nvidia.com>", "target": 0, "func": "static void rtrs_clt_dev_release(struct device *dev)\n{\n\tstruct rtrs_clt_sess *clt = container_of(dev, struct rtrs_clt_sess,\n\t\t\t\t\t\t dev);\n\n\tmutex_destroy(&clt->paths_ev_mutex);\n\tmutex_destroy(&clt->paths_mutex);\n\tkfree(clt);\n}", "func_hash": 280791422344483990445186805760516988921, "file_name": "rtrs-clt.c", "file_hash": 123062054699569722257369072160789726422, "cwe": ["CWE-415"], "cve": "CVE-2022-29156", "cve_desc": "drivers/infiniband/ulp/rtrs/rtrs-clt.c in the Linux kernel before 5.16.12 has a double free related to rtrs_clt_dev_release.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29156"}
{"idx": 344242, "project": "lua", "commit_id": "42d40581dd919fb134c07027ca1ce0844c670daf", "project_url": "https://github.com/lua/lua", "commit_url": "https://github.com/lua/lua/commit/42d40581dd919fb134c07027ca1ce0844c670daf", "commit_message": "Save stack space while handling errors\n\nBecause error handling (luaG_errormsg) uses slots from EXTRA_STACK,\nand some errors can recur (e.g., string overflow while creating an\nerror message in 'luaG_runerror', or a C-stack overflow before calling\nthe message handler), the code should use stack slots with parsimony.\n\nThis commit fixes the bug \"Lua-stack overflow when C stack overflows\nwhile handling an error\".", "target": 0, "func": "l_noret luaG_runerror (lua_State *L, const char *fmt, ...) {\n  CallInfo *ci = L->ci;\n  const char *msg;\n  va_list argp;\n  luaC_checkGC(L);  /* error message uses memory */\n  va_start(argp, fmt);\n  msg = luaO_pushvfstring(L, fmt, argp);  /* format message */\n  va_end(argp);\n  if (isLua(ci)) {  /* if Lua function, add source:line information */\n    luaG_addinfo(L, msg, ci_func(ci)->p->source, getcurrentline(ci));\n    setobjs2s(L, L->top - 2, L->top - 1);  /* remove 'msg' from the stack */\n    L->top--;\n  }\n  luaG_errormsg(L);\n}", "func_hash": 213182302759646008213936194965010984301, "file_name": "None", "file_hash": null, "cwe": ["CWE-787"], "cve": "CVE-2022-33099", "cve_desc": "An issue in the component luaG_runerror of Lua v5.4.4 and below leads to a heap-buffer overflow when a recursive error occurs.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-33099"}
{"idx": 387593, "project": "linux", "commit_id": "5934d9a0383619c14df91af8fd76261dc3de2f5f", "project_url": "https://github.com/torvalds/linux", "commit_url": "https://github.com/torvalds/linux/commit/5934d9a0383619c14df91af8fd76261dc3de2f5f", "commit_message": "ALSA: control: Re-order bounds checking in get_ctl_id_hash()\n\nThese two checks are in the reverse order so it might read one element\nbeyond the end of the array.  First check if the \"i\" is within bounds\nbefore using it.\n\nFixes: 6ab55ec0a938 (\"ALSA: control: Fix an out-of-bounds bug in get_ctl_id_hash()\")\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nLink: https://lore.kernel.org/r/YwjgNh/gkG1hH7po@kili\nSigned-off-by: Takashi Iwai <tiwai@suse.de>", "target": 0, "func": "static unsigned long get_ctl_id_hash(const struct snd_ctl_elem_id *id)\n{\n\tint i;\n\tunsigned long h;\n\n\th = id->iface;\n\th = MULTIPLIER * h + id->device;\n\th = MULTIPLIER * h + id->subdevice;\n\tfor (i = 0; i < SNDRV_CTL_ELEM_ID_NAME_MAXLEN && id->name[i]; i++)\n\t\th = MULTIPLIER * h + id->name[i];\n\th = MULTIPLIER * h + id->index;\n\th &= LONG_MAX;\n\treturn h;\n}", "func_hash": 34537843047622620480136251678937271731, "file_name": "control.c", "file_hash": 333017582912688268799367809880673073354, "cwe": ["CWE-125"], "cve": "CVE-2022-3170", "cve_desc": "An out-of-bounds access issue was found in the Linux kernel sound subsystem. It could occur when the 'id->name' provided by the user did not end with '\\0'. A privileged local user could pass a specially crafted name through ioctl() interface and crash the system or potentially escalate their privileges on the system.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-3170"}
{"idx": 506696, "project": "openssl", "commit_id": "97ab3c4b538840037812c8d9164d09a1f4bf11a1", "project_url": "https://github.com/openssl/openssl", "commit_url": "https://github.com/openssl/openssl/commit/97ab3c4b538840037812c8d9164d09a1f4bf11a1", "commit_message": "Add a test for GENERAL_NAME_cmp\n\nBased on a boringssl test contributed by David Benjamin\n\nReviewed-by: Tomas Mraz <tmraz@fedoraproject.org>", "target": 0, "func": "int setup_tests(void)\n{\n    ADD_ALL_TESTS(call_run_cert, OSSL_NELEM(name_fns));\n    ADD_TEST(test_GENERAL_NAME_cmp);\n    return 1;\n}", "func_hash": 300508003271249901349188890485879920788, "file_name": "None", "file_hash": null, "cwe": ["CWE-476"], "cve": "CVE-2020-1971", "cve_desc": "The X.509 GeneralName type is a generic type for representing different types of names. One of those name types is known as EDIPartyName. OpenSSL provides a function GENERAL_NAME_cmp which compares different instances of a GENERAL_NAME to see if they are equal or not. This function behaves incorrectly when both GENERAL_NAMEs contain an EDIPARTYNAME. A NULL pointer dereference and a crash may occur leading to a possible denial of service attack. OpenSSL itself uses the GENERAL_NAME_cmp function for two purposes: 1) Comparing CRL distribution point names between an available CRL and a CRL distribution point embedded in an X509 certificate 2) When verifying that a timestamp response token signer matches the timestamp authority name (exposed via the API functions TS_RESP_verify_response and TS_RESP_verify_token) If an attacker can control both items being compared then that attacker could trigger a crash. For example if the attacker can trick a client or server into checking a malicious certificate against a malicious CRL then this may occur. Note that some applications automatically download CRLs based on a URL embedded in a certificate. This checking happens prior to the signatures on the certificate and CRL being verified. OpenSSL's s_server, s_client and verify tools have support for the \"-crl_download\" option which implements automatic CRL downloading and this attack has been demonstrated to work against those tools. Note that an unrelated bug means that affected versions of OpenSSL cannot parse or construct correct encodings of EDIPARTYNAME. However it is possible to construct a malformed EDIPARTYNAME that OpenSSL's parser will accept and hence trigger this attack. All OpenSSL 1.1.1 and 1.0.2 versions are affected by this issue. Other OpenSSL releases are out of support and have not been checked. Fixed in OpenSSL 1.1.1i (Affected 1.1.1-1.1.1h). Fixed in OpenSSL 1.0.2x (Affected 1.0.2-1.0.2w).", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2020-1971"}
